---
title: "myvignette2"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{myvignette2}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(StatComp22037)
```

# HW0 (A-22037-2022-09-09) 

## Question

Question 1: Go through “R for Beginners” if you are not familiar with R programming.



Question 2:Use knitr to produce at least 3 examples (texts, figures, tables).

## Answer

Answer 1: Thanks for the advice, yet I've been familiar with R, could finish the next exercise at least.

Answer 2:
```{r eval=FALSE, include=FALSE}
setwd("D:/mathbooks/multianalysis")
```

```{r eval=FALSE, include=FALSE}
install.packages("car")
install.packages("carData")
```

# From Applied multivariate statistical analysis(Johnson, Richard Arnold), 6ed.



#Example 2: 4.30
```{r}
data_4.30_x1<-c(1,2,3,3,4,5,6,8,9,11)
data_4.30_x2<-c(18.95,19.00,17.95,15.54,14.00,12.95,8.94,7.49,6.00,3.99)
```

```{r echo=TRUE}
## (Box-Cox)
box_cox<-function(x,lambda){
  if(lambda==0)
    { 
    return(log(x))
  }else{
    return((x^lambda-1)/lambda)
  }
}
l_value<-function(X,lamda){
  x_new<-sapply(X,box_cox,lambda=lamda)
  x_bar<-mean(x_new)
  l_val<-log(mean((x_new-x_bar)^2))*(-length(x_new)/2)+(lamda-1)*sum(log(X))
  return(l_val)
}

lambda<-seq(-1,2,1e-04)
all_l<-c()
for(n in 1:length(lambda)){
  all_l[n]<-l_value(data_4.30_x1,lamda=lambda[n])
}

max_lambda<-lambda[which(all_l==max(all_l))]
max_lambda

new_data<-sapply(data_4.30_x1,box_cox,lambda=max_lambda)

qqnorm(new_data)
```

```{r}
box_cox<-function(x,lambda)
{
  if(lambda==0)
  {
    return(log(x))
  }
  else
  {
    return((x^lambda-1)/lambda)
  }
}
```


```{r}
###################################
#(b)
#The same from (a)
lambda<-seq(-1,2,1e-04)
all_l<-c()
for(n in 1:length(lambda)){
  all_l[n]<-l_value(data_4.30_x2,lamda=lambda[n])
}

max_lambda<-lambda[which(all_l==max(all_l))]
max_lambda
new_data<-sapply(data_4.30_x2,box_cox,lambda=max_lambda)

qqnorm(new_data)
```

#(c)

```{r}
library(carData)
library(car)
```

```{r}
data_4.30<-cbind(data_4.30_x1,data_4.30_x2)
```


```{r}
powerTransform(data_4.30~1)
```



```{r echo=FALSE}
fun1<-function(x,y){
  if(x==0)
  {
    print("Wrong!")
    return(0)
  }
    
else  return(y/x)
}
```

```{r eval=FALSE, include=FALSE}
t0<-record_temp$V1
t1<-sapply((record_temp$V2),y=100,fun1)
t2<-sapply((record_temp$V3),y=200,fun1)
t3<-sapply((record_temp$V4),y=400,fun1)
t4<-sapply((record_temp$V5),y=(800/60),fun1)
t5<-sapply((record_temp$V6),y=(1500/60),fun1)
t6<-sapply((record_temp$V7),y=(3000/60),fun1)
t7<-sapply((record_temp$V8),y=(42195/60),fun1)
```

```{r eval=FALSE, include=FALSE}
record_tem<-cbind(t0,t1,t2,t3,t4,t5,t6,t7)
record<-as.data.frame(record_tem)
colnames(record)<-c("country","100m(m/s)","200m(m/s)","400m(m/s)","800m(m/s)","1500m(m/s)","3000m(m/s)","Marathon(m/s)")
```

```{r eval=FALSE, include=FALSE}
record_tem<-cbind(t1,t2,t3,t4,t5,t6,t7)
record<-as.data.frame(record_tem)
colnames(record)<-c("100m(m/s)","200m(m/s)","400m(m/s)","800m(m/s)","1500m(m/s)","3000m(m/s)","Marathon(m/s)")
```

#marginal normality
```{r}

norm_test<-function(data){
  
  new_data<-sort(data)
  len_data<-length(new_data)
  prob<-function(i,n){#
    return((i-0.5)/n)
  }
  
  all_pro<-sapply(1:len_data,prob,n=len_data)
  
  all_q<-qnorm(all_pro)
 
  return(cor(new_data,all_q))
}
```




Notice the values of rQ decrease with increasing distance. In
this case, as the distance increases,the distribution of times becomes increasingly skewed to the left.




##Example 4: Package kableExtra(For the table example)

```{r}
library("kableExtra")
dt <- mtcars[1:5, 1:6]
kbl(dt)
```

# HW1 (A-22037-2022-09-15)

## Question

# Question 1: Ex 3.3

# Question 2: Ex 3.7

# Question 3: Ex 3.12

# Question 4: Ex 3.13

## Answer

# Answer 1

# (1)求反函数。

$u=F(x)=1-(b/x)^a\Rightarrow x=b(1-u)^{-1/a}.$

```{r}
# set.seed函数的意义是，确定唯一一组随机数，保证结果可复现
set.seed(0)
a=2
b=2
n=1000
u<-runif(n)
x<-b*u^(-1/a)
```

# (2)作直方图和密度函数图。

$X的密度函数f(x)=F'(x)=ab^ax^{-(a+1)},x\ge b.$
```{r}
hist(x, breaks = "Scott", prob = TRUE)#作出直方图

y<-sort(x)
fx<- a*b^a*y^(-(a+1))#导数
lines(y, fx)#作出密度函数图
```

# Answer 2

# (1)用接受-拒绝法生成beta随机数。

# 从均匀分布U(0,1)生成n个随机数x。由接受-拒绝法，当$x^{a-1}(1-x)^{b-1}>u时接受x。$

```{r}
myrbeta<-function(n, a, b) {#自定义函数生成beta随机数
n=1000
k=0
y=numeric(n)
while(k<n){
u<-runif(1)
x<-runif(1)
if (x^(a-1) *(1-x)^(b-1)>u) {
  k<-k+1
  y[k]<-x
 }
}
 return(y)
}
```

# (2)作直方图和密度函数图。


```{r}
set.seed(1)
y <- myrbeta(1000,a=3,b=2)
hist(y, breaks = "Scott", prob = TRUE, ylim = c(0, 2))#直方图
t <- seq(0, 1, 0.01)
ft <- 12*t^2*(1-t)#标准Beta(3,2)函数
lines(t,ft)#密度曲线
```

# Answer 3

$\Lambda$服从$Gamma(r,\beta),$ 同时Y服从均值为$\Lambda$的指数分布。因此，以下通过gamma(4,2)随机抽样得到1000个$\lambda$的值，再生成1000个随机数X.
```{r}
set.seed(2)
n=1000
r=4
beta=2
lambda<-rgamma(n, r, beta)
x<-rexp(n, rate = lambda)
```

```{r eval=FALSE, include=FALSE}
#附带作图，聊博一乐
hist(x, breaks = "Scott", prob = TRUE)#作出直方图
y<-sort(x)
lines(x,y)
```


# 附注：对rexp中rate项的解释：

# 在?rexp的官方文档中，rate被解释为"vector of rates", 即指数分布期望\lambda的倒数组成的向量, 其中元素不妨仍记为rate。而关于样本数量n和向量维数m的大小关系，有如下结论：

# (1)若n=m, 则每一个rate生成一个随机数x;

# (2)若n>m, 则每一个rate生成一个随机数x后，重复这一过程，直到达到样本数量n;

# (3)若n<m，则取前n个rate生成一个随机数x.

# Answer 4

# 数据来自上一题。

$由Pareto分布的表达式，可知其pdf$
$$f(y)=F'(y)=\frac{r\beta^r}{(\beta+y)^{r+1}},\hspace{.8em}y\geq 0.$$

# 与Question 1和2类似，先生成1000个随机数，作出模拟数据直方图；再利用真实数据作出密度函数曲线。
```{r}
hist(x, breaks = "Scott", prob = TRUE)
 y<-sort(x)
 fy<-r*beta^r*(beta + y)^(-(r+1))
 lines(y, fy)
```

# HW2 (A-22037-2022-09-22)

I divide the homework into three problems.

## Question

# Q1: 

# (1)For  $n =10^4, 2\times10^4, 4\times10^4, 6\times 10^4, 8\times 10^4,$ apply the fast sorting algorithm to randomly permuted numbers of $1\cdots n$.

# (2)Calculate computation time averaged over 100 simulations, denoted by $a_n$.

# (3)Regress $a_n$ on $t_n:=n\log n$, and graphically show the results (scatter plot and regression line).

# Q2: Ex 5.6 on textbook.

# Q3: Ex 5.7 on textbook.

##Answer

# Answer 1

(1)(2)
First, we define the function of fast sort.
```{r}
quickSort <- function(arr) {
# Pick a number at random and its index.
mid.val <- sample(seq_along(arr),1); mid <- arr[mid.val]
arr <- arr[-mid.val]
# Place-holders for left and right values.
left <- c()
right <- c()
# Move all the smaller and equal values to the left, bigger values to the right.
left<-arr[which(arr<=mid)]
right<-arr[which(arr>mid)]
if (length(left) > 1) {
left <- quickSort(left)
}
if (length(right) > 1) {
right <- quickSort(right)
}
# Finally, return the sorted values.
return(c(left, mid, right))
}
```

Next, we apply the sort function to randomly permuted numbers in different sample size, which are generated by function "sample".
```{r include=FALSE}
n<-c(10^4,2*10^4,4*10^4,6*10^4,8*10^4)#sample size
t=100#repeated times

```

```{r}
set.seed(1)
an<-rep(NA,length(n))#the sequence to record the mean
a=rep(NA,100)#the sequence recording each sort's time
for(i in 1:100)
{
  time1=Sys.time()
quickSort(sample(1:n[1],n[1]))#permuate random numbers each time, and sort them
a[i]=Sys.time()-time1
}
an[1]=mean(a)
```

```{r include=FALSE}
set.seed(2)
a=rep(NA,100)
for(i in 1:100)
{
  time1=Sys.time()
quickSort(sample(1:n[2],n[2]))
a[i]=Sys.time()-time1
}
an[2]=mean(a)
```

```{r include=FALSE}
set.seed(3)
a=rep(NA,100)
for(i in 1:100)
{
  time1=Sys.time()
quickSort(sample(1:n[3],n[3]))
a[i]=Sys.time()-time1
}
an[3]=mean(a)
```

```{r include=FALSE}
set.seed(4)
a=rep(NA,100)
for(i in 1:100)
{
  time1=Sys.time()
quickSort(sample(1:n[4],n[4]))
a[i]=Sys.time()-time1
}
an[4]=mean(a)
```

```{r include=FALSE}
set.seed(5)
a=rep(NA,100)
for(i in 1:100)
{
  time1=Sys.time()
quickSort(sample(1:n[5],n[5]))
a[i]=Sys.time()-time1
}
an[5]=mean(a)
```

(3)Finally, we show the results graphically.
```{r}
tn=n*log(n)
lm(an~tn)
```


```{r}
library(ggplot2)
data=as.data.frame(cbind(an,tn))
ggplot(data,aes(x=an,y=tn))+geom_point(shape=19)+xlab("tn := n log(n)")+ylab("averaged time cost")+geom_smooth(method = lm,level=0.95)#apply ggplot to show the results graphically
```

# Answer 2

First, $e^U$'s expecation $E(e^U)=\int_0^1 e^u\mathbb{d}x=e-1;$ similarly, we could get $E(e^{1-U})=e-1, E(e^{2U})=\frac{1}{2}(e^2-1).$

Second, From the formula $\Cov (X,Y)=E(XY)-EXEY,$ we get 

$$\begin{align}

\mathit{Cov}(e^U,e^{1-U})&=E(e^Ue^{1-U})-E(e^U)E(e^{1-U})\\
&=e-(e-1)^2\doteq-0.23421;
\end{align}$$

$$\begin{align}

Var(e^U)&=E(e^{2U})-(E(e^U))^2\\
&=\frac{1}{2}(e^2-1)-(e-1)^2\\
&=2e-\frac{1}{2}e^2-\frac{3}{2}\doteq 0.24203.
\end{align}$$

Similarly, $Var(e^{1-U})=2e-\frac{1}{2}e^2-\frac{3}{2}.$

Thus, 

$$\begin{align}

Var(e^U+e^{1-U})&=Var(e^U)+Var(e^{1-U})+2Cov(e^U,e^{1-U})\\
&=2e-\frac{1}{2}e^2-\frac{3}{2}+2e-\frac{1}{2}e^2-\frac{3}{2}+2[e-(e-1)^2]\\
&=10e-3e^2-5\doteq 0.015662.
\end{align}$$

Suppose $\hat{\theta}_1$ is the simple MC estimator, and $\hat{\theta}_2$ is the antithetic estimator. Then if U and V i.i.d~U(0,1), we have 

$$Var(\hat{\theta}_1)=Var(\frac{1}{2}(e^U+e^V))=\frac{1}{2}Var(e^U)=\frac{1}{2}(2e-\frac{1}{2}e^2-\frac{3}{2})\doteq0.12102.$$ If antithetic variables are used, then 
$$\begin{align}
Var(\hat{\theta}_2)=Var(\frac{1}{2}(e^U+e^{1-U}))&=\frac{1}{2}(2Var(e^U)+2Cov(e^U,e^{1-U}))\\
&\doteq0.0039126.
\end{align}$$

So the reduction in variance is $$\frac{var(\hat{\theta}_1)-Var(\hat\theta_2)}{var(\hat{\theta}_1)}=\frac{0.12102-0.0039126}{0.12102}=0.9677=96.77\%.$$


# Answer 3

```{r}
set.seed(201)
m=10000
mc <-replicate(1000, expr = {mean(exp(runif(m)))})#simple MC method
anti <- replicate(1000, expr = {u <- runif(m/2) 
v <-1-u 
mean((exp(u) + exp(v))/2)
})#antithetic variate approach

```

```{r}
v1 <- var(mc)
v2 <- var(anti)
c(mean(mc), mean(anti))
```

```{r}
c(v1, v2)
```

```{r}
(v1-v2)/v1#reduction percent
```

  In this simulation, the reduction in variance shown on the last line above is
0.9652, close to the theoretical value 0.9677 from Ex2.

# HW3 (A-22037-2022-09-30)

## Question

# Question 1: Ex 5.13

# Question 2: Ex 5.15

## Answer

# Answer 1

First display the graph of g(x). From the graph, we might consider a normal distribution or a gamma distribution.


```{r}
x <- seq(1, 10, 0.01)
y <- x^2 * exp(-x^2/2)/sqrt(2 * pi)#standard normal distribution N(0,1), g(x)
plot(x, y, type = "l", ylim = c(0, 1))
lines(x, 2 * dnorm(x, 1), lty = 2)#2*N(0,1)
lines(x, dgamma(x - 1, 3/2, 2), lty = 3)#Gamma (1.5,2)
legend("topright", inset = 0.02, legend = c("g(x)", "f1","f2"), lty = 1:3)
```

Here $f_1$ is N(0,1)'s pdf that is translated to x > 1. Thus $f_1$ is twice the
N(1, 1) density. The gamma variable is also translated to x > 1.These two satisfy the conditions that the support set is $(1,\infty)$.

Now we compare the ratios of $g(x)/f_i(x),\hspace{.3em} i=1,2.$
```{r}
plot(x, y/(dgamma(x - 1, 3/2, 2)), type = "l", lty = 3,ylab = "")
lines(x, y/(2 * dnorm(x, 1)), lty = 2)
legend("topright", inset = 0.02, legend = c("f1", "f2"),lty = 2:3)
```

From the plot, we might expect the folded normal importance function to produce the smaller variance in estimating the integral, because the ratio g(x)/f(x) is
closer to a constant function.


## Answer 2

The part needed modifying is the subintervals. Actually, according to Page 147, "The interior endpoints are the percentiles or quantiles." Therefore, instead of $(\frac{j-1}{5},\frac{j}{5}), j=1,\dots 5$ , we should use intervals $(a_i,a_{i+1}),i=0,\dots,4$, where $a_0=0,a_j=F^{-1}(j/5),j=1,\dots,4,a_5=1$ , F(x) is the primitive function of $f_3(x)$.

Since $f_3(x)=\frac{e^{-x}}{1-e^{-1}},\hspace{.3em}0<x<1$ , we could get $F(x)=\frac{1-e^{-x}}{1-e^{-1}},\hspace{.3em}0<x<1$. Then we get $F^{-1}(u)=-\ln[1-(1-e^{-1})u]$.

```{r}
M <- 10000
k <- 5
m <- M/k
si <- numeric(k)
v <- numeric(k)
```

```{r}
g <- function(x) exp(-x)/(1 + x^2)#g(x)
f <- function(x) (k/(1 - exp(-1))) * exp(-x)#$f_3(x)$
for (j in 1:k) {
u <- runif(m, (j - 1)/k, j/k)
x <- -log(1 - (1 - exp(-1)) * u)#Applying $F^{-1}(u)$ to calculate the quantiles
gf <- g(x)/f(x)
si[j] <- mean(gf)
v[j] <- var(gf)
}
```

```{r}
sum(si)
```

```{r}
mean(v)
```

```{r}
sqrt(mean(v))
```

The below codes show the without stratified situation. Compared with it, we have a smaller variance by stratification:
```{r}
k <- 1
m <- M/k
si <- numeric(k)
v <- numeric(k)
for (j in 1:k) {
u <- runif(m, (j - 1)/k, j/k)
x <- -log(1 - (1 - exp(-1)) * u)
fg <- g(x)/f(x)
si[j] <- mean(fg)
v[j] <- var(fg)
}

sum(si)

mean(v)

sqrt(mean(v))
```


## HW4 (A-22037-2022-09-30)

##Question

#Q1: Ex 6.4 on textbook.

#Q2: Ex 6.8 on textbook.

#Q3: (i)If we obtain the powers for two methods under a particular simulation setting with 10,000 experiments: say, 0.651 for one method and 0.676 for another method. Can we say the powers are different at 0.05 level?

#(ii)What is the corresponding hypothesis test problem?

#(iii)Which test can we use? Z-test, two-sample t-test, paired-t test or McNemar test? Why?

#(iv)Please provide the least necessary information for hypothesis testing.

##Answer

#Answer 1

Since population is lognormal, we transform X to normal and estimate $\mu$ with the sample mean of the transformed sample.


```{r}
n <- 30
CI <- replicate(10000, expr = {
x <- rlnorm(n)
y <- log(x)
ybar <- mean(y)
se <- sd(y)/sqrt(n)
ybar + se * qnorm(c(0.025, 0.975))
})
LCL <- CI[1, ]
UCL <- CI[2, ]
```

```{r}
sum(LCL < 0 & UCL > 0)
```

```{r}
mean(LCL < 0 & UCL > 0)
```

#Answer 2

```{r}
count5test <- function(x, y) {
X <- x - mean(x)
Y <- y - mean(y)
outx <- sum(X > max(Y)) + sum(X < min(Y))
outy <- sum(Y > max(X)) + sum(Y < min(X))
return(as.integer(max(c(outx, outy)) > 5))
}
```

```{r}
sigma1 <- 1
sigma2 <- 1.5
m <- 10000
for (n in c(20, 30, 50, 100, 200, 500)) {
tests <- replicate(m, expr = {
x <- rnorm(n, 0, sigma1)
y <- rnorm(n, 0, sigma2)
C5 <- count5test(x, y)
Fp <- var.test(x, y)$p.value
 Ftest <- as.integer(Fp <= 0.055)
c(C5, Ftest)
 })
cat(n, rowMeans(tests), "\n")
}
```

#Answer 3

Firstly, the corresponding hypothesis test problem is:
$$H_0:p_2-p_1=0\leftrightarrow H_1:p_2-p_1\neq 0,\hspace{1em}\alpha=0.05$$

where $X_1\sim b_1(n_1,p_1),\hspace{.3em}X_2\sim b_2(n_2,p_2),\hspace{.3em}n_1=n_2=10^4.$

Since the sample size is large, from CLT, We could apply Z-test for the question. The Z-statistic is defined as$$Z=\dfrac{\bar{Y}-\bar{X}-(p_2-p_1)}{\sqrt{\dfrac{p_1(1-p_1)}{n_1}+\dfrac{p_2(1-p_2)}{n_2}}}\xrightarrow{L}N(0,1),$$where the rejection field is $D=\{|Z|>\dfrac{\alpha}{2}\}.$ If $H_0$ is true, then $p_1=p_2=p,$ thus $\hat{p}=(p_1+p_2)/2=0.6635.$ From the calculation we get $|Z|=3.741>u_{\frac{\alpha}{2}}=1.96, $ hence we reject $H_0$ at 0.05 level, that is to say, the powers are different at 0.05 level.

## HW5 (A-22037-2022-10-14)

## Question

# Question 1: Ex 7.4

# Question 2: Ex 7.5

# Question 3: Ex 7.A

##Answer 

# Answer 1

It is easy to calculate that the MLE of $\lambda$ is $1/\bar{x}$. The estimates of bias and standard error are printed in the summary of boot below.

```{r}
library(boot)#for dataset data set aircondit; in the 7.A we'll also use the function boot(), boot.ci()
set.seed(101)
```

```{r}
aircondit#dataset
R=5e3#replication times
```

```{r}
x <- aircondit[1]#extract the data
haz.rate <- function(y, j) {return(1/mean(as.matrix(y[j, ])))}#define the function calculating rate
boot(x, statistic = haz.rate, R)#bootstrap calculation
```

# Answer 2

Applying the data in Answer 1.
```{r}
haz.mean<- function(y, j) {return(mean(as.matrix(y[j, ])))}#define the function calculating mean
```

```{r}
boot(x, statistic = haz.mean, R)#bootstrap calculation
```

```{r}
temp<-boot(x, statistic = haz.mean, R)
boot.ci(temp, type = c("norm", "perc", "basic", "bca"))#other four methods to get the Interval
```

The replicates are not approximately normal, so the normal and percentile
intervals differ.

```{r}
hist(temp$t, prob = TRUE, main = "")
points(temp$t0, 0, cex = 2, pch = 16)
```


From the histogram of replicates, it appears that the distribution of the replicates is skewed - although we are estimating a mean, the sample size is too small for CLT to give a good approximation here. The BCa interval is a percentile type interval, but it adjusts for both skewness and bias.


##HW6 (A-22037-2022-10-21)

##Question

#Q1: Ex 7.8

#Q2: Ex 7.11

#Q3: Ex 8.2

##Answer

#Answer 1
```{r}
library(bootstrap)
```

```{r}
x <- as.matrix(scor)
n <- nrow(x)
jacktheta <- numeric(n)#initialized dataset
lambda <- eigen(cov(x))$values
 theta.hat<-max(lambda/sum(lambda))#About theta's definition, please refer to Ex7.7
for (i in 1:n) {
y <- x[-i, ]#delete the ith data, to apply the jackknife
s <- cov(y)
lambda <- eigen(s)$values
jacktheta[i] <- max(lambda/sum(lambda))
}
jack.bias<- (n - 1) * (mean(jacktheta) - theta.hat)#bias' jackknife estimation
jack.se <- sqrt((n - 1)/n * sum((jacktheta- mean(jacktheta))^2))#standard's jackknife estimation
c(theta.hat, jack.bias, jack.se)
```

The jackknife estimate of bias of $\hat{\theta}$ is approximately 0.001 and the jackknife estimate of se is approximately 0.05. These estimates are not very different from the bootstrap estimates above.

#Answer 2

```{r include=FALSE}
library(DAAG)
attach(ironslag)
```

```{r eval=FALSE, include=FALSE}
#Firstly, review the code in Ex 7.18.
attach(ironslag)
n <- length(magnetic) #in DAAG ironslag
e1 <- e2 <- e3 <- e4 <- numeric(n)
# for n-fold cross validation
# fit models on leave-one-out samples
for (k in 1:n) {
y <- magnetic[-k]
x <- chemical[-k]

J1 <- lm(y ~ x)
yhat1 <- J1$coef[1] + J1$coef[2] * chemical[k]
e1[k] <- magnetic[k] - yhat1

J2 <- lm(y ~ x + I(x^2))
yhat2 <- J2$coef[1] + J2$coef[2] * chemical[k] +
J2$coef[3] * chemical[k]^2
e2[k] <- magnetic[k] - yhat2

J3 <- lm(log(y) ~ x)
logyhat3 <- J3$coef[1] + J3$coef[2] * chemical[k]
yhat3 <- exp(logyhat3)
e3[k] <- magnetic[k] - yhat3

J4 <- lm(log(y) ~ log(x))
logyhat4 <- J4$coef[1] + J4$coef[2] * log(chemical[k])
yhat4 <- exp(logyhat4)
e4[k] <- magnetic[k] - yhat4
}
```

```{r eval=FALSE, include=FALSE}
 c(mean(e1^2), mean(e2^2), mean(e3^2), mean(e4^2))
```


```{r}
n <- length(magnetic)
N <- choose(n, 2)
e1 <- e2 <- e3 <- e4 <- e5 <- numeric(N)
ind <- 1
for (i in 1:(n - 1)) for (j in (i + 1):n) {
k <- c(i, j)
y <- magnetic[-k]
x <- chemical[-k]
J1 <- lm(y ~ x)
yhat1 <- J1$coef[1] + J1$coef[2] * chemical[k]
e1[ind] <- sum((magnetic[k] - yhat1)^2)#first model: linear

J2 <- lm(y ~ x + I(x^2))
 yhat2 <- J2$coef[1] + J2$coef[2] * chemical[k] +
 J2$coef[3] * chemical[k]^2
e2[ind] <- sum((magnetic[k] - yhat2)^2)#second model: quadratic

J3 <- lm(log(y) ~ x)
logyhat3 <- J3$coef[1] + J3$coef[2] * chemical[k]
yhat3 <- exp(logyhat3)
e3[ind] <- sum((magnetic[k] - yhat3)^2)#third model:  Exponential

J4 <- lm(log(y) ~ log(x))
logyhat4 <- J4$coef[1] + J4$coef[2] * log(chemical[k])
yhat4 <- exp(logyhat4)
e4[ind] <- sum((magnetic[k] - yhat4)^2)#fourth model: log-log

ind=ind+1
}
```



```{r}
c(sum(e1), sum(e2), sum(e3), sum(e4))/N#Attention: shouldn't divide the length(magnetic); instead, get the mean toward all possible situations
```

```{r}
detach(ironslag)
```

The quadratic model (2) is again selected according to the minimum prediction
error by leave-two-out cross-validation.

#Answer 3
```{r}
spear.permu <- function(x, y) {
stest <- cor.test(x, y, method = "spearman")#x,y's spearman correlation
n <- length(x)
rs <- replicate(R, expr = {
k <- sample(1:n)
cor.test(x, y[k], method = "spearman")$estimate
})
rs1 <- c(stest$estimate, rs)
pval <- mean(as.integer(stest$estimate <=rs1))
return(list(rho.s = stest$estimate, p.value = pval))
}

```

In the following examples, the mvrnorm function is used to generate correlated
samples. 
```{r}
library(MASS)
mu <- c(0, 0)
Sigma <- matrix(c(1, 0.75, 0.75, 1), 2, 2)
n <- 30
R <- 499
x <- mvrnorm(n, mu, Sigma)
```

```{r}
cor.test(x[, 1], x[, 2], method = "spearman")
```

```{r}
spear.permu(x[, 1], x[, 2])
```

The p-values in the test are both significant and close in value.

## HW7 (A-22037-2022-10-28)


## HW8 (A-22037-2022-11-04)

```{r setup11, fig.height=10, fig.width=10, echo=T, eval=T}
set.seed(123)

# The function to generate the random sample
RSample <- function(n,alpha,beta){
  X <- runif(n,10,20)
  gamma <- 1;aM <- 0.5;aY <- 1
  M <- aM+alpha*X+rnorm(n)
  Y <- aY+beta*M+gamma*X+rnorm(n)
  return(list(X,M,Y))
}

# The function of test statistics computation
Ttest <- function(X,M,Y){
  fit1 <- summary(lm(M~X))
  fit2 <- summary(lm(Y~X+M))
  a <- fit1$coefficients[2,1]
  sea <- fit1$coefficients[2,2]
  b <- fit2$coefficients[3,1]
  seb <- fit2$coefficients[3,2]
  return(a*b/((a*seb)^2+(b*sea)^2)^0.5)
}

# The function to implement the test hypothesis
Imptest <- function(N,n,X,M,Y,T0){
  T1 <- T2 <- T3 <- numeric(N)
  # Condition 1
  for(i in 1:N){
    n1 <- sample(1:n, size=n, replace=FALSE)
    n2 <- sample(1:n, size=n, replace=FALSE)
    X1 <- X[n1];M1 <- M[n2];Y1 <- Y[n2]
    T1[i] <- Ttest(X1,M1,Y1)
  }
  # Condition 2
  for(i in 1:N){
    n1 <- sample(1:n, size = n, replace = FALSE)
    n2 <- sample(1:n, size = n, replace = FALSE)
    X2 <- X[n1];M2 <- M[n1];Y2 <- Y[n2]
    T2[i] <- Ttest(X2,M2,Y2)
  }
  # Condition 3
  for(i in 1:N){
    n1 <- sample(1:n, size = n, replace = FALSE)
    n2 <- sample(1:n, size = n, replace = FALSE)
    M3 <- M[n1];X3 <- X[n2];Y3 <- Y[n2]
    T3[i] <- Ttest(X3,M3,Y3)
  }
  # The p-value of Condition1
  p1 <- mean(abs(c(T0,T1))>abs(T0))
  # The p-value of Condition2
  p2 <- mean(abs(c(T0,T2))>abs(T0))
  # The p-value of Condition3
  p3 <- mean(abs(c(T0,T3))>abs(T0))
  return(c(p1,p2,p3))
}

N <- 1000 # The number of simulation
n <- 100 # The number of random sample
T0 <- numeric(3)
p <- matrix(0,3,3)
# The real values of parameters
alpha <- c(0,0,1);beta <- c(0,1,0)

for(i in 1:3){
  result <- RSample(n,alpha[i],beta[i])
  X <- result[[1]]
  M <- result[[2]]
  Y <- result[[3]]
  # The original value of test statistics
  T0[i] <- Ttest(X,M,Y)
  p[i,] <- Imptest(N,n,X,M,Y,T0[i])
}
```

Output the table of p-values for the permutation tests above.

```{r setup12, fig.height=10, fig.width=10, echo=T, eval=T}
# Result reporting
colnames(p) <- c("Condition 1","Condition 2","Condition 3")
rownames(p) <- c("alpha=0,beta=0","alpha=0,beta=1","alpha=1,beta=0")
p

# Clean the memory of the variables
rm(list=ls())
```

According to the output, none of the permutation tests under the three conditions can control the type I error very well.

Ex2
```{r}
library("locfit")
```


```{r}
myfun.expit<-function(N,b1,b2,b3,f0){
  x1<-rep(1,N)
  x2<-rexp(N,1)
  x3<-rbinom(N,1,0.5)
  alpha<-expit(f0+b1*x1+b2*x2+b3*x3)
  return(alpha)
}
```

```{r}
alpha1<-myfun.expit(1e6,0,1,-1,.1)
```

## HW9 (A-22037-2022-11-11)

$(2)$
```{r}
#Direct maximization
# import data
n = 10
temp = matrix(data=c(11,12,8,9,27,28,13,14,16,17,0,1,23,24,10,11,24,25,2,3),ncol = 2,nrow = 10,byrow = TRUE)
```


```{r}
set.seed(103)
logL_partial = function(lambda){#log-likelihood function's partial, so we could get the mle by 
  s = 0
  for (i in 1:n){
    u = temp[i,1]
    v = temp[i,2]
    s = s + (v*exp(-lambda*v)-u*exp(-lambda*u))/(exp(-lambda*u)-exp(-lambda*v))
  }
  return(s)
}
solution2 = uniroot(logL_partial,c(0,10))
unlist(solution2)[1]
```

```{r}
#EM
```


# 2.1.3

# Ex4

As opposed to as.vector(), unlist() could flatten the nested structure, which means its result is an one-dimensional array.

# Ex5

These operators are all functions which coerce their arguments(in these cases)to character, double and character. To enlighten the latter case: “one” comes after “2” in ASCII. So "one" > 2 is true.

# 2.3.1 

# Ex1

The result is NULL. For example:
```{r}
dim(c(1,2,3,6))
```

# Ex2

TRUE. You could get the following texts from ?array:

"A two-dimensional array is the same thing as a matrix."


# 2.4.5

# Ex1

Names, row.names and class.

# Ex2

You could get the following texts from ?as.matrix:

"The method for data frames will return a character matrix if there is only atomic columns and any non-(numeric/logical/complex) column, applying as.vector to factors and format to other non-character columns. Otherwise, the usual coercion hierarchy (logical < integer < double < complex) will be used, e.g., all-logical data frames will be coerced to a logical matrix, mixed logical-integer will give a integer matrix, etc."

# Ex3

Yes, you can create them easily. Also both dimensions can be 0:
```{r}
#The data give the speed of cars and the distances taken to stop. Note that the data were recorded in the 1920s. 
cars[FALSE,]#data frame with 0 rows
cars[,FALSE]#data frame with 0 columns
cars[F,F]#data frame with 0 rows and 0 columns
```

#HW10 (A-22037-2022-11-18)

## Question

# Question 1: Ex11.1.2 2

# Question 2: Ex11.2.5 1

# Question 3: 

# Implement a Gibbs sampler to generate a bivariate normal chain (Xt, Yt) with zero means, unit standard deviations, and correlation 0.9.
# • Write an Rcpp function.
# • Compare the corresponding generated random numbers with pure R language using the function “qqplot”.
# • Compare the computation time of the two functions with the function “microbenchmark”.

## Answer

```{r include=FALSE}
data(package = 'datasets')#to get the common datasets in r
```


# Answer 1

```{r}
scale01 <- function(x) {
rng <- range(x, na.rm = TRUE)
(x - rng[1]) / (rng[2] - rng[1])
}
```

Since this function needs numeric input, one can check this via an if clause. If one also wants to return non-numeric input columns, these can be supplied to the else argument of the if() “function”:

```{r}
data.frame(lapply(cars, function(x) if (is.numeric(x)) scale01(x) else x))
data.frame(lapply(iris, function(x) if (is.numeric(x)) scale01(x) else x))[1:20,]
```

# Answer 2

(1)

As a numeric dataframe we choose cars:

```{r}
vapply(cars, sd, numeric(1))
```

For the mixed dataframe we choose iris:

```{r}
vapply(iris[vapply(iris, is.numeric, logical(1))],sd, numeric(1))
```


# Answer 3

# (1)
```{r eval=FALSE, include=FALSE}
sourceCpp(paste0(dir_cpp,"gibbsC.cpp"))
```


# (2)
## Gibbs algorithm
```{r eval=FALSE, include=FALSE}
    #initialize constants and parameters
    N <- 5000               #length of chain
    burn <- 1000            #burn-in length
    X <- matrix(0, N, 2)    #the chain, a bivariate sample

    rho <- -.75             #correlation
    mu1 <- 0
    mu2 <- 2
    sigma1 <- 1
    sigma2 <- .5
    s1 <- sqrt(1-rho^2)*sigma1
    s2 <- sqrt(1-rho^2)*sigma2

    ###### generate the chain #####

    X[1, ] <- c(mu1, mu2)            #initialize

    for (i in 2:N) {
        x2 <- X[i-1, 2]
        m1 <- mu1 + rho * (x2 - mu2) * sigma1/sigma2
        X[i, 1] <- rnorm(1, m1, s1)
        x1 <- X[i, 1]
        m2 <- mu2 + rho * (x1 - mu1) * sigma2/sigma1
        X[i, 2] <- rnorm(1, m2, s2)
    }

    b <- burn + 1
    x <- X[b:N, ]
```

```{r eval=FALSE, include=FALSE}
Gibbs_in_r<-function(N,burn,mu1,mu2,sigma1,sigma2,rho){
  #initialize constants and parameters
    #N: length of chain
    #burn: burn-in length
    #rho: correlation
  X <- matrix(0, N, 2)    #the chain, a bivariate sample
  s1 <- sqrt(1-rho^2)*sigma1
  s2 <- sqrt(1-rho^2)*sigma2
    
    ###### generate the chain #####

  X[1, ] <- c(mu1, mu2)            #initialize

  for (i in 2:N) {
        x2 <- X[i-1, 2]
        m1 <- mu1 + rho * (x2 - mu2) * sigma1/sigma2
        X[i, 1] <- rnorm(1, m1, s1)
        x1 <- X[i, 1]
        m2 <- mu2 + rho * (x1 - mu1) * sigma2/sigma1
        X[i, 2] <- rnorm(1, m2, s2)
    }

  b <- burn + 1
  x <- X[b:N, ]
  return(x)
}
```


```{r}
temp1<-gibbsC(5000,2,0,0,1,1,.9)
temp2<-Gibbsinr(5000,0,0,0,1,1,.9)
```

```{r}
qqplot(temp1[,1],temp2[,1])
qqplot(temp1[,2],temp2[,2])
```

There seems no significant difference between two results.


# (3)
```{r}
library(microbenchmark)
```

```{r}
ts<-microbenchmark(gibbsC(5000,2,0,0,1,1,.9),Gibbsinr(5000,0,0,0,1,1,.9))
summary(ts)[,c(1,3,4,5,6)]
```

However, the computation time of the C++ functions is much faster than in R.
